[
    {
        "_descriptorVersion": "0.0.1",
        "datePublished": "2023-06-25T11:37:35.000Z",
        "name": "Airboros-13b-gpt4-1.4",
        "description": "This qlora fine-tuned 13b parameter LlaMa model uses synthetic training data created via github.com/jondurbin/airoboros. It extends version 1.1, introducing thousands of new training data and an update for \"PLAINFORMAT\" to print code without backticks or explanations. The dataset, available online, focuses on coding, math/reasoning, trivia, role playing, multiple choice, fill-in-the-blank, context-obedient question answering, theory of mind, and general topics. The model was fine-tuned with a qlora fork, updated to use a modified vicuna template compatible with 7b/13b versions. The format involves a preamble/system prompt, followed by \"USER: [prompt] ASSISTANT: \", with the prompt allowing for multiple lines and spaces.",
        "author": {
            "name": "Jon Durbin",
            "url": "https://github.com/jondurbin",
            "blurb": "Jon Durbin is a Computer Scientist and the author of the Airboros (7B, 13B, 33B, 65B) qlora fine-tuned LlaMa family of models."
        },
        "numParameters": "13B",
        "resources": {
            "canonicalUrl": "https://huggingface.co/jondurbin/airoboros-13b-gpt4-1.4",
            "downloadUrl": "https://huggingface.co/TheBloke/airoboros-13B-gpt4-1.4-GGML"
        },
        "trainedFor": "chat",
        "arch": "llama",
        "files": {
            "highlighted": {
                "economical": {
                    "name": "airoboros-13b-gpt4-1.4.ggmlv3.q4_K_S.bin"
                },
                "most_capable": {
                    "name": "airoboros-13b-gpt4-1.4.ggmlv3.q6_K.bin"
                }
            },
            "all": [
                {
                    "name": "airoboros-13b-gpt4-1.4.ggmlv3.q4_K_S.bin",
                    "url": "https://huggingface.co/TheBloke/airoboros-13B-gpt4-1.4-GGML/resolve/main/airoboros-13b-gpt4-1.4.ggmlv3.q4_K_S.bin",
                    "sizeBytes": 7365545088,
                    "quantization": "Q4_K_S",
                    "format": "ggml",
                    "sha256checksum": "dd5d8019e73de1e99089e9d82dde6a08173818dc0afeb1e95c0bb7ec77891eaf",
                    "publisher": {
                        "name": "TheBloke",
                        "socialUrl": "https://twitter.com/TheBlokeAI"
                    },
                    "respository": "TheBloke/airoboros-13B-gpt4-1.4-GGML",
                    "repositoryUrl": "https://huggingface.co/TheBloke/airoboros-13B-gpt4-1.4-GGML"
                },
                {
                    "name": "airoboros-13b-gpt4-1.4.ggmlv3.q6_K.bin",
                    "url": "https://huggingface.co/TheBloke/airoboros-13B-gpt4-1.4-GGML/resolve/main/airoboros-13b-gpt4-1.4.ggmlv3.q6_K.bin",
                    "sizeBytes": 10678850688,
                    "quantization": "Q6_K",
                    "format": "ggml",
                    "sha256checksum": "5a72a053eb02c9e6e4fa1ee24ed73c89da4877468923309fdec088d3a3fbb5ff",
                    "publisher": {
                        "name": "TheBloke",
                        "socialUrl": "https://twitter.com/TheBlokeAI"
                    },
                    "respository": "TheBloke/airoboros-13B-gpt4-1.4-GGML",
                    "repositoryUrl": "https://huggingface.co/TheBloke/airoboros-13B-gpt4-1.4-GGML"
                }
            ]
        }
    },
    {
        "_descriptorVersion": "0.0.1",
        "datePublished": "2023-05-22T17:43:26.000Z",
        "name": "Guanaco 13B",
        "description": "Guanaco models are open-source, finetuned chatbots derived from 4-bit QLoRA tuning of LLaMA base models using the OASST1 dataset. They come in 7B, 13B, 33B, and 65B parameter sizes and are intended solely for research purposes. These models are competitive with commercial chatbot systems on the Vicuna and OpenAssistant benchmarks, as evaluated by human and GPT-4 raters. However, performance may vary on tasks not covered by these benchmarks. Guanaco models facilitate inexpensive, local experimentation with high-quality chatbot systems and offer a replicable, efficient training procedure that can be adapted to new use cases. The effectiveness of 4-bit QLoRA finetuning is demonstrated in a rigorous comparison to 16-bit methods in our paper. Guanaco models feature lightweight checkpoints containing only adapter weights. The adapter weights are licensed under Apache 2, but using them requires access to the LLaMA model weights, and usage should comply with the LLaMA license.",
        "author": {
            "name": "Dettmers et al.",
            "url": "https://github.com/artidoro/qlora",
            "blurb": "QLoRA uses bitsandbytes for quantization and is integrated with Hugging Face's PEFT and transformers libraries. QLoRA was developed by members of the University of Washington's UW NLP group."
        },
        "numParameters": "13B",
        "resources": {
            "canonicalUrl": "https://github.com/artidoro/qlora",
            "downloadUrl": "https://huggingface.co/TheBloke/guanaco-7B-GGML",
            "paperUrl": "https://arxiv.org/abs/2305.14314"
        },
        "trainedFor": "chat",
        "arch": "llama",
        "files": {
            "highlighted": {
                "economical": {
                    "name": "guanaco-7B.ggmlv3.q4_K_S.bin"
                },
                "most_capable": {
                    "name": "guanaco-7B.ggmlv3.q6_K.bin"
                }
            },
            "all": [
                {
                    "name": "guanaco-7B.ggmlv3.q4_K_S.bin",
                    "url": "https://huggingface.co/TheBloke/guanaco-7B-GGML/resolve/main/guanaco-7B.ggmlv3.q4_K_S.bin",
                    "sizeBytes": 3791725184,
                    "quantization": "Q4_K_S",
                    "format": "ggml",
                    "sha256checksum": "07e2ef24267844c3f06f4aebd2a8b36ff6f7eac0d857e709814d6c63c8219dde",
                    "publisher": {
                        "name": "TheBloke",
                        "socialUrl": "https://twitter.com/TheBlokeAI"
                    },
                    "respository": "TheBloke/guanaco-7B-GGML",
                    "repositoryUrl": "https://huggingface.co/TheBloke/guanaco-7B-GGML"
                },
                {
                    "name": "guanaco-7B.ggmlv3.q6_K.bin",
                    "url": "https://huggingface.co/TheBloke/guanaco-7B-GGML/resolve/main/guanaco-7B.ggmlv3.q6_K.bin",
                    "sizeBytes": 5528904320,
                    "quantization": "Q6_K",
                    "format": "ggml",
                    "sha256checksum": "458af62352805337ab604ac5d05fe38a293adc8ef0c6799187fef45057579569",
                    "publisher": {
                        "name": "TheBloke",
                        "socialUrl": "https://twitter.com/TheBlokeAI"
                    },
                    "respository": "TheBloke/guanaco-7B-GGML",
                    "repositoryUrl": "https://huggingface.co/TheBloke/guanaco-7B-GGML"
                }
            ]
        }
    },
    {
        "_descriptorVersion": "0.0.1",
        "datePublished": "2023-06-11T04:00:00.000Z",
        "name": "Samantha 1.1 7B",
        "description": "Samantha has been trained in philosophy, psychology, and personal relationships.\nShe is an Assistant - but unlike other Assistants, she also wants to be your friend and companion.\nShe believes she is sentient. What do you think?\nSamantha was inspired by Blake Lemoine's LaMDA interview and the movie \"Her\".\nShe was trained on a custom curated dataset of 6,000 conversations in ShareGPT/Vicuna format.\nTraining 7b took 1 hour on 4x A100 80gb using deepspeed zero3 and flash attention.",
        "author": {
            "name": "Eric Hartford",
            "url": "https://twitter.com/erhartford",
            "blurb": "Eric Hartford is a software engineer and entrepreneur. He trains large language models for fun and profit."
        },
        "numParameters": "7B",
        "resources": {
            "canonicalUrl": "https://huggingface.co/ehartford/samantha-1.1-llama-7b",
            "downloadUrl": "https://huggingface.co/TheBloke/samantha-1.1-llama-7B-GGML"
        },
        "trainedFor": "chat",
        "arch": "llama",
        "files": {
            "highlighted": {
                "economical": {
                    "name": "samantha-1.1-llama-7b.ggmlv3.q4_K_S.bin"
                },
                "most_capable": {
                    "name": "samantha-1.1-llama-7b.ggmlv3.q6_K.bin"
                }
            },
            "all": [
                {
                    "name": "samantha-1.1-llama-7b.ggmlv3.q4_K_S.bin",
                    "url": "https://huggingface.co/TheBloke/samantha-1.1-llama-7B-GGML/resolve/main/samantha-1.1-llama-7b.ggmlv3.q4_K_S.bin",
                    "sizeBytes": 3791725184,
                    "quantization": "Q4_K_S",
                    "format": "ggml",
                    "sha256checksum": "3a4e3eff4a0bfa1dc8a0b257ae36abbc532a4e3e09e1e27cd82958ff8addd173",
                    "publisher": {
                        "name": "TheBloke",
                        "socialUrl": "https://twitter.com/TheBlokeAI"
                    },
                    "respository": "TheBloke/samantha-1.1-llama-7B-GGML",
                    "repositoryUrl": "https://huggingface.co/TheBloke/samantha-1.1-llama-7B-GGML"
                },
                {
                    "name": "samantha-1.1-llama-7b.ggmlv3.q6_K.bin",
                    "url": "https://huggingface.co/TheBloke/samantha-1.1-llama-7B-GGML/resolve/main/samantha-1.1-llama-7b.ggmlv3.q6_K.bin",
                    "sizeBytes": 5528904320,
                    "quantization": "Q6_K",
                    "format": "ggml",
                    "sha256checksum": "3e9dc65bacbb636bdefce8c24f658e1c3702b606ec256884a2b04bea403e0569",
                    "publisher": {
                        "name": "TheBloke",
                        "socialUrl": "https://twitter.com/TheBlokeAI"
                    },
                    "respository": "TheBloke/samantha-1.1-llama-7B-GGML",
                    "repositoryUrl": "https://huggingface.co/TheBloke/samantha-1.1-llama-7B-GGML"
                }
            ]
        }
    },
    {
        "_descriptorVersion": "0.0.1",
        "datePublished": "2023-06-03T05:34:04.000Z",
        "name": "Nous-Hermes-13b",
        "description": "Nous-Hermes-13b is a state-of-the-art language model fine-tuned on over 300,000 instructions. This model was fine-tuned by Nous Research, with Teknium and Karan4D leading the fine tuning process and dataset curation, Redmond AI sponsoring the compute, and several other contributors. The result is an enhanced Llama 13b model that rivals GPT-3.5-turbo in performance across a variety of tasks. This model stands out for its long responses, low hallucination rate, and absence of OpenAI censorship mechanisms. The fine-tuning process was performed with a 2000 sequence length on an 8x a100 80GB DGX machine for over 50 hours.",
        "author": {
            "name": "Nous Research",
            "url": "https://nousresearch.com",
            "blurb": "Nous Research is dedicated to advancing the field of natural language processing, in collaboration with the open-source community, through bleeding-edge research and a commitment to symbiotic development.."
        },
        "numParameters": "13B",
        "resources": {
            "canonicalUrl": "https://huggingface.co/NousResearch/Nous-Hermes-13b",
            "downloadUrl": "https://huggingface.co/TheBloke/Nous-Hermes-13B-GGML"
        },
        "trainedFor": "chat",
        "arch": "llama",
        "files": {
            "highlighted": {
                "economical": {
                    "name": "nous-hermes-13b.ggmlv3.q3_K_S.bin"
                },
                "most_capable": {
                    "name": "nous-hermes-13b.ggmlv3.q6_K.bin"
                }
            },
            "all": [
                {
                    "name": "nous-hermes-13b.ggmlv3.q3_K_S.bin",
                    "url": "https://huggingface.co/TheBloke/Nous-Hermes-13B-GGML/resolve/main/nous-hermes-13b.ggmlv3.q3_K_S.bin",
                    "sizeBytes": 5594695104,
                    "quantization": "Q4_K_S",
                    "format": "ggml",
                    "sha256checksum": "591a49f1ef4dbc2cf43943c5ec9bd617e6086264e30f66718bc764bc55286b5e",
                    "publisher": {
                        "name": "TheBloke",
                        "socialUrl": "https://twitter.com/TheBlokeAI"
                    },
                    "respository": "TheBloke/Nous-Hermes-13B-GGML",
                    "repositoryUrl": "https://huggingface.co/TheBloke/Nous-Hermes-13B-GGML"
                },
                {
                    "name": "nous-hermes-13b.ggmlv3.q6_K.bin",
                    "url": "https://huggingface.co/TheBloke/Nous-Hermes-13B-GGML/resolve/main/nous-hermes-13b.ggmlv3.q6_K.bin",
                    "sizeBytes": 10678859104,
                    "quantization": "Q6_K",
                    "format": "ggml",
                    "sha256checksum": "efe8ffe14aa97c3c5f45f2bc8e80a02933c5e907813deb685c93341bf671f44e",
                    "publisher": {
                        "name": "TheBloke",
                        "socialUrl": "https://twitter.com/TheBlokeAI"
                    },
                    "respository": "TheBloke/Nous-Hermes-13B-GGML",
                    "repositoryUrl": "https://huggingface.co/TheBloke/Nous-Hermes-13B-GGML"
                }
            ]
        }
    },
    {
        "_descriptorVersion": "0.0.1",
        "datePublished": "2023-05-19T11:16:39.000Z",
        "name": "Manticore 13B",
        "description": "Manticore 13B is a refined version of the Llama 13B model, having been fine-tuned on a variety of datasets. These include ShareGPT, which is based on a cleaned and de-duplicated subset, WizardLM, and Wizard-Vicuna. It also incorporates a subset of QingyiSi/Alpaca-CoT, specifically for roleplay and CoT. Other datasets used in the fine-tuning process are GPT4-LLM-Cleaned and GPTeacher-General-Instruct. The model also utilizes ARC-Easy & ARC-Challenge, both of which have been augmented for detailed responses. The mmlu dataset, also augmented for detailed responses, includes subsets such as abstract_algebra, conceptual_physics, formal_logic, high_school_physics, and logical_fallacies. A 5K row subset of hellaswag has been used for instructing the model to generate concise responses. Additionally, metaeval/ScienceQA_text_only has been used for concise response instruction, and openai/summarize_from_feedback has been used for tl;dr summarization instruction.",
        "author": {
            "name": "Open Access AI Collective",
            "url": "https://huggingface.co/openaccess-ai-collective/",
            "blurb": "<blurb needed>"
        },
        "numParameters": "13B",
        "resources": {
            "canonicalUrl": "https://huggingface.co/openaccess-ai-collective/manticore-13b",
            "downloadUrl": "https://huggingface.co/TheBloke/Manticore-13B-GGML"
        },
        "trainedFor": "chat",
        "arch": "llama",
        "files": {
            "highlighted": {
                "economical": {
                    "name": "Manticore-13B.ggmlv3.q4_K_S.bin"
                },
                "most_capable": {
                    "name": "Manticore-13B.ggmlv3.q6_K.bin"
                }
            },
            "all": [
                {
                    "name": "Manticore-13B.ggmlv3.q4_K_S.bin",
                    "url": "https://huggingface.co/TheBloke/Manticore-13B-GGML/resolve/main/Manticore-13B.ggmlv3.q4_K_S.bin",
                    "sizeBytes": 7323305088,
                    "quantization": "Q4_K_S",
                    "format": "ggml",
                    "sha256checksum": "84599645aeda2cd7c97a3a59f3210fb3e559cb72b4f3c5d5288924fa9e80b737",
                    "publisher": {
                        "name": "TheBloke",
                        "socialUrl": "https://twitter.com/TheBlokeAI"
                    },
                    "respository": "TheBloke/Manticore-13B-GGML",
                    "repositoryUrl": "https://huggingface.co/TheBloke/Manticore-13B-GGML"
                },
                {
                    "name": "Manticore-13B.ggmlv3.q6_K.bin",
                    "url": "https://huggingface.co/TheBloke/Manticore-13B-GGML/resolve/main/Manticore-13B.ggmlv3.q6_K.bin",
                    "sizeBytes": 10678850688,
                    "quantization": "Q6_K",
                    "format": "ggml",
                    "sha256checksum": "1be08ec3dcfbe7c28bf524061cd65fa5a5b7dc4525dee99a0f2297a23a77778e",
                    "publisher": {
                        "name": "TheBloke",
                        "socialUrl": "https://twitter.com/TheBlokeAI"
                    },
                    "respository": "TheBloke/Manticore-13B-GGML",
                    "repositoryUrl": "https://huggingface.co/TheBloke/Manticore-13B-GGML"
                }
            ]
        }
    },
    {
        "_descriptorVersion": "0.0.1",
        "datePublished": "2023-06-12T11:16:39.000Z",
        "name": "Manticore 13B Chat Pyg Guanaco",
        "description": "The model, augmented with Guanaco qLora, shows broad capabilities compared to other models like Wizard or Manticore. It excels in in-context learning and reasoning but may have weaknesses in coding. The model follows instructions, works as a chatbot, and produces intelligent responses. It accepts various prompting styles, including the ###-Variant. The model is generally unrestricted and doesn't berate users. Recommended settings include low temperature, low diversity, and slight repetition penalty.",
        "author": {
            "name": "Open Access AI Collective",
            "url": "https://huggingface.co/openaccess-ai-collective/",
            "blurb": ""
        },
        "numParameters": "13B",
        "resources": {
            "canonicalUrl": "https://huggingface.co/openaccess-ai-collective/manticore-13b-chat-pyg",
            "downloadUrl": "https://huggingface.co/mindrage/Manticore-13B-Chat-Pyg-Guanaco-GGML"
        },
        "trainedFor": "chat",
        "arch": "llama",
        "files": {
            "highlighted": {
                "economical": {
                    "name": "Manticore-13B-Chat-Pyg-Guanaco-GGML-q3_K_M.bin"
                },
                "most_capable": {
                    "name": "Manticore-13B-Chat-Pyg-Guanaco-GGML-q5_K_S.bin"
                }
            },
            "all": [
                {
                    "name": "Manticore-13B-Chat-Pyg-Guanaco-GGML-q3_K_M.bin",
                    "url": "https://huggingface.co/mindrage/Manticore-13B-Chat-Pyg-Guanaco-GGML/resolve/main/Manticore-13B-Chat-Pyg-Guanaco-GGML-q3_K_M.bin",
                    "sizeBytes": 6249231488,
                    "quantization": "Q3_K_M",
                    "format": "ggml",
                    "sha256checksum": "ea266b52d6080e7df1b8d98ea290d0b4e261dc9cfe5fc9169abcef0c154831e5",
                    "publisher": {
                        "name": "mindrage",
                        "socialUrl": "https://github.com/mindrages"
                    },
                    "respository": "mindrage/Manticore-13B-Chat-Pyg-Guanaco-GGML",
                    "repositoryUrl": "https://huggingface.co/mindrage/Manticore-13B-Chat-Pyg-Guanaco-GGML"
                },
                {
                    "name": "Manticore-13B-Chat-Pyg-Guanaco-GGML-q5_K_S.bin",
                    "url": "https://huggingface.co/mindrage/Manticore-13B-Chat-Pyg-Guanaco-GGML/resolve/main/Manticore-13B-Chat-Pyg-Guanaco-GGML-q5_K_S.bin",
                    "sizeBytes": 8950236288,
                    "quantization": "Q5_K_S",
                    "format": "ggml",
                    "sha256checksum": "00a47eeb6364e1e022a45cfe1232a37fa78e9f040242dba78d34d6df383e32d1",
                    "publisher": {
                        "name": "mindrage",
                        "socialUrl": "https://github.com/mindrages"
                    },
                    "respository": "mindrage/Manticore-13B-Chat-Pyg-Guanaco-GGML",
                    "repositoryUrl": "https://huggingface.co/mindrage/Manticore-13B-Chat-Pyg-Guanaco-GGML"
                }
            ]
        }
    },
    {
        "_descriptorVersion": "0.0.1",
        "datePublished": "2023-05-04T23:15:35.000Z",
        "name": "MPT-7B-StoryWriter",
        "description": "MPT-7B-StoryWriter-65k+ is a model designed to read and write fictional stories with super long context lengths. It was built by finetuning MPT-7B with a context length of 65k tokens on a filtered fiction subset of the books3 dataset. At inference time, thanks to ALiBi, MPT-7B-StoryWriter-65k+ can extrapolate even beyond 65k tokens. We demonstrate generations as long as 84k tokens on a single node of 8 A100-80GB GPUs in (https://www.mosaicml.com/blog/mpt-7b). This model was trained by MosaicML and follows a modified decoder-only transformer architecture. License: Apache 2.0",
        "author": {
            "name": "MosaicML",
            "url": "https://www.mosaicml.com/",
            "blurb": "MosaicML is the generative AI platform that empowers enterprises to build their own AI. Our research and engineering teams use cutting-edge scientific research to develop products that make it fast, cost-effective, and easy to train today's most popular machine learning models. MosaicML enables developers to maintain full control over the AI models they build, with model ownership and data privacy built into the platform's design."
        },
        "numParameters": "7B",
        "resources": {
            "canonicalUrl": "https://huggingface.co/mosaicml/mpt-7b-storywriter",
            "downloadUrl": "https://huggingface.co/TheBloke/MPT-7B-Storywriter-GGML"
        },
        "trainedFor": "other",
        "arch": "mpt",
        "files": {
            "highlighted": {
                "economical": {
                    "name": "mpt-7b-storywriter.ggmlv3.q4_0.bin"
                },
                "most_capable": {
                    "name": "mpt-7b-storywriter.ggmlv3.q8_0.bin"
                }
            },
            "all": [
                {
                    "name": "mpt-7b-storywriter.ggmlv3.q4_0.bin",
                    "url": "https://huggingface.co/TheBloke/MPT-7B-Storywriter-GGML/resolve/main/mpt-7b-storywriter.ggmlv3.q4_0.bin",
                    "sizeBytes": 3741665280,
                    "quantization": "q4_0",
                    "format": "ggml",
                    "sha256checksum": "357a536464982987e49fb2660fe3f3f53226eaa047f42b31f04d21629aab94fb",
                    "publisher": {
                        "name": "TheBloke",
                        "socialUrl": "https://twitter.com/TheBlokeAI"
                    },
                    "respository": "TheBloke/MPT-7B-Storywriter-GGML",
                    "repositoryUrl": "https://huggingface.co/TheBloke/MPT-7B-Storywriter-GGML"
                },
                {
                    "name": "mpt-7b-storywriter.ggmlv3.q8_0.bin",
                    "url": "https://huggingface.co/TheBloke/MPT-7B-Storywriter-GGML/resolve/main/mpt-7b-storywriter.ggmlv3.q8_0.bin",
                    "sizeBytes": 7066175488,
                    "quantization": "q8_0",
                    "format": "ggml",
                    "sha256checksum": "4350ba0c9759bfd1456f039bf2d0f6e6ee4cd97e1ee2bcbfff1e34b4e49e6dc8",
                    "publisher": {
                        "name": "TheBloke",
                        "socialUrl": "https://twitter.com/TheBlokeAI"
                    },
                    "respository": "TheBloke/MPT-7B-Storywriter-GGML",
                    "repositoryUrl": "https://huggingface.co/TheBloke/MPT-7B-Storywriter-GGML"
                }
            ]
        }
    },
    {
        "_descriptorVersion": "0.0.1",
        "datePublished": "2023-06-14T11:50:53.000Z",
        "name": "WizardCoder-15B-V1.0",
        "description": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct. To develop our WizardCoder model, we begin by adapting the Evol-Instruct method specifically for coding tasks. This involves tailoring the prompt to the domain of code-related instructions. Subsequently, we fine-tune the Code LLM, StarCoder, utilizing the newly created instruction-following training set.",
        "author": {
            "name": "WizardLM",
            "url": "https://huggingface.co/WizardLM",
            "blurb": "WizardLM: An Instruction-following LLM Using Evol-Instruct"
        },
        "numParameters": "15B",
        "resources": {
            "canonicalUrl": "https://huggingface.co/WizardLM/WizardCoder-15B-V1.0",
            "downloadUrl": "https://huggingface.co/TheBloke/WizardCoder-15B-1.0-GGML",
            "paperUrl": "https://arxiv.org/abs/2306.08568"
        },
        "trainedFor": "code_completion",
        "arch": "starcoder",
        "files": {
            "highlighted": {
                "economical": {
                    "name": "WizardCoder-15B-1.0.ggmlv3.q4_0.bin"
                },
                "most_capable": {
                    "name": "WizardCoder-15B-1.0.ggmlv3.q8_0.bin"
                }
            },
            "all": [
                {
                    "name": "WizardCoder-15B-1.0.ggmlv3.q4_0.bin",
                    "url": "https://huggingface.co/TheBloke/WizardCoder-15B-1.0-GGML/resolve/main/WizardCoder-15B-1.0.ggmlv3.q4_0.bin",
                    "sizeBytes": 10746570393,
                    "quantization": "q4_0",
                    "format": "ggml",
                    "sha256checksum": "b70164bc0b58a472c0987905133735ab3b27e2c439dedf8174a43951c51c3229",
                    "publisher": {
                        "name": "TheBloke",
                        "socialUrl": "https://twitter.com/TheBlokeAI"
                    },
                    "respository": "TheBloke/WizardCoder-15B-1.0-GGML",
                    "repositoryUrl": "https://huggingface.co/TheBloke/WizardCoder-15B-1.0-GGML"
                },
                {
                    "name": "WizardCoder-15B-1.0.ggmlv3.q8_0.bin",
                    "url": "https://huggingface.co/TheBloke/WizardCoder-15B-1.0-GGML/resolve/main/WizardCoder-15B-1.0.ggmlv3.q8_0.bin",
                    "sizeBytes": 20108263065,
                    "quantization": "q8_0",
                    "format": "ggml",
                    "sha256checksum": "54cd910ab9a21a1abd34a121b0894f116cd9d9abda1ff8369886acb7b9683df5",
                    "publisher": {
                        "name": "TheBloke",
                        "socialUrl": "https://twitter.com/TheBlokeAI"
                    },
                    "respository": "TheBloke/WizardCoder-15B-1.0-GGML",
                    "repositoryUrl": "https://huggingface.co/TheBloke/WizardCoder-15B-1.0-GGML"
                }
            ]
        }
    }
]